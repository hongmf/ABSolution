"""
ABSDataLoader: Data loading utility for ABS (Asset-Backed Securities) analytics
Supports both AWS data sources and mock data for development/testing
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Optional, Dict, List
import boto3
from botocore.exceptions import ClientError
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ABSDataLoader:
    """
    Data loader for ABS analytics platform
    Supports loading data from AWS services (S3, Athena) or using mock data
    """

    def __init__(self, use_mock_data: bool = False, **kwargs):
        """
        Initialize ABSDataLoader

        Args:
            use_mock_data: If True, use mock data instead of AWS services
            **kwargs: Additional configuration options
                - s3_bucket: S3 bucket name for data storage
                - athena_database: Athena database name
                - region: AWS region
        """
        self.use_mock_data = use_mock_data
        self.s3_bucket = kwargs.get('s3_bucket', 'abs-analytics-data')
        self.athena_database = kwargs.get('athena_database', 'abs_analytics')
        self.region = kwargs.get('region', 'us-east-1')

        # Initialize AWS clients only if not using mock data
        if not self.use_mock_data:
            try:
                self.s3_client = boto3.client('s3', region_name=self.region)
                self.athena_client = boto3.client('athena', region_name=self.region)
                logger.info("AWS clients initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize AWS clients: {e}")
                logger.info("Falling back to mock data mode")
                self.use_mock_data = True

    def load_filings_data(self) -> pd.DataFrame:
        """
        Load SEC filings data

        Returns:
            DataFrame with normalized SEC ABS filings
        """
        if self.use_mock_data:
            return self._generate_mock_filings()
        else:
            return self._load_filings_from_s3()

    def load_issuer_data(self) -> pd.DataFrame:
        """
        Load issuer information

        Returns:
            DataFrame with issuer details
        """
        if self.use_mock_data:
            return self._generate_mock_issuers()
        else:
            return self._load_issuers_from_s3()

    def load_pool_performance(self) -> pd.DataFrame:
        """
        Load loan pool performance metrics

        Returns:
            DataFrame with pool performance data
        """
        if self.use_mock_data:
            return self._generate_mock_pool_performance()
        else:
            return self._load_pool_performance_from_s3()

    def load_risk_scores(self) -> pd.DataFrame:
        """
        Load risk scores generated by ML models

        Returns:
            DataFrame with risk scores
        """
        if self.use_mock_data:
            return self._generate_mock_risk_scores()
        else:
            return self._load_risk_scores_from_s3()

    # Mock data generation methods
    def _generate_mock_filings(self) -> pd.DataFrame:
        """Generate mock SEC filings data for testing"""
        np.random.seed(42)
        n_records = 100

        issuers = ['Ford Credit', 'American Express', 'Capital One', 'Synchrony', 'GM Financial']
        asset_classes = ['Auto Loan', 'Credit Card', 'Student Loan', 'Equipment Lease']
        form_types = ['ABS-EE', '10-D', '10-K', '8-K']

        base_date = datetime.now() - timedelta(days=730)

        data = {
            'accession_number': [f'0001{str(i).zfill(6)}-{str(23+i%2)}-{str(i%100).zfill(6)}'
                                for i in range(n_records)],
            'filing_date': [base_date + timedelta(days=i*7) for i in range(n_records)],
            'issuer_name': np.random.choice(issuers, n_records),
            'asset_class': np.random.choice(asset_classes, n_records),
            'form_type': np.random.choice(form_types, n_records),
            'deal_name': [f'Deal-{i//5 + 1}' for i in range(n_records)],
            'original_balance': np.random.uniform(100_000_000, 2_000_000_000, n_records),
            'current_balance': np.random.uniform(50_000_000, 1_500_000_000, n_records),
            'delinquency_rate': np.random.uniform(0.01, 0.15, n_records),
            'average_fico': np.random.randint(620, 780, n_records),
            'quality_score': np.random.choice([0.3, 0.7, 1.0], n_records, p=[0.1, 0.3, 0.6])
        }

        df = pd.DataFrame(data)
        df['filing_year'] = df['filing_date'].dt.year
        df['filing_quarter'] = df['filing_date'].dt.quarter

        return df

    def _generate_mock_issuers(self) -> pd.DataFrame:
        """Generate mock issuer data"""
        issuers = [
            {'issuer_id': 1, 'issuer_name': 'Ford Credit', 'cik': '38009',
             'primary_asset_class': 'Auto Loan', 'total_deals': 15},
            {'issuer_id': 2, 'issuer_name': 'American Express', 'cik': '4962',
             'primary_asset_class': 'Credit Card', 'total_deals': 22},
            {'issuer_id': 3, 'issuer_name': 'Capital One', 'cik': '927628',
             'primary_asset_class': 'Credit Card', 'total_deals': 18},
            {'issuer_id': 4, 'issuer_name': 'Synchrony', 'cik': '1601712',
             'primary_asset_class': 'Credit Card', 'total_deals': 12},
            {'issuer_id': 5, 'issuer_name': 'GM Financial', 'cik': '1529377',
             'primary_asset_class': 'Auto Loan', 'total_deals': 10}
        ]

        return pd.DataFrame(issuers)

    def _generate_mock_pool_performance(self) -> pd.DataFrame:
        """Generate mock pool performance data"""
        np.random.seed(42)
        n_records = 200

        base_date = datetime.now() - timedelta(days=730)

        data = {
            'deal_name': [f'Deal-{i%20 + 1}' for i in range(n_records)],
            'reporting_date': [base_date + timedelta(days=i*15) for i in range(n_records)],
            'pool_balance': np.random.uniform(50_000_000, 1_000_000_000, n_records),
            'total_principal': np.random.uniform(1_000_000, 50_000_000, n_records),
            'total_interest': np.random.uniform(100_000, 10_000_000, n_records),
            'delinquencies_30_days': np.random.uniform(0.01, 0.05, n_records),
            'delinquencies_60_days': np.random.uniform(0.005, 0.03, n_records),
            'delinquencies_90_days': np.random.uniform(0.002, 0.02, n_records),
            'charge_offs': np.random.uniform(0.001, 0.015, n_records),
            'prepayment_rate': np.random.uniform(0.05, 0.25, n_records),
            'weighted_avg_coupon': np.random.uniform(3.5, 8.5, n_records)
        }

        return pd.DataFrame(data)

    def _generate_mock_risk_scores(self) -> pd.DataFrame:
        """Generate mock risk scores from ML model"""
        np.random.seed(42)
        n_records = 100

        deals = [f'Deal-{i+1}' for i in range(20)]

        data = {
            'deal_name': np.random.choice(deals, n_records),
            'score_date': [datetime.now() - timedelta(days=i) for i in range(n_records)],
            'risk_score': np.random.uniform(0.1, 0.9, n_records),
            'risk_category': np.random.choice(['Low', 'Medium', 'High'], n_records, p=[0.5, 0.35, 0.15]),
            'delinquency_forecast': np.random.uniform(0.01, 0.20, n_records),
            'confidence_level': np.random.uniform(0.7, 0.95, n_records)
        }

        df = pd.DataFrame(data)
        df['alert_triggered'] = df['risk_score'] > 0.7

        return df

    # AWS data loading methods
    def _load_filings_from_s3(self) -> pd.DataFrame:
        """Load filings data from S3/Athena"""
        try:
            # Query Athena for normalized filings
            query = f"""
            SELECT *
            FROM {self.athena_database}.normalized_sec_filings
            WHERE filing_date >= DATE_ADD('month', -24, CURRENT_DATE)
            """

            response = self._execute_athena_query(query)
            return pd.DataFrame(response)

        except Exception as e:
            logger.error(f"Error loading filings from S3: {e}")
            logger.info("Falling back to mock data")
            return self._generate_mock_filings()

    def _load_issuers_from_s3(self) -> pd.DataFrame:
        """Load issuer data from S3"""
        try:
            query = f"""
            SELECT DISTINCT
                issuer_name,
                cik,
                asset_class as primary_asset_class,
                COUNT(DISTINCT deal_name) as total_deals
            FROM {self.athena_database}.normalized_sec_filings
            GROUP BY issuer_name, cik, asset_class
            """

            response = self._execute_athena_query(query)
            df = pd.DataFrame(response)
            df['issuer_id'] = range(1, len(df) + 1)

            return df

        except Exception as e:
            logger.error(f"Error loading issuers from S3: {e}")
            return self._generate_mock_issuers()

    def _load_pool_performance_from_s3(self) -> pd.DataFrame:
        """Load pool performance from S3"""
        try:
            query = f"""
            SELECT *
            FROM {self.athena_database}.pool_performance
            WHERE reporting_date >= DATE_ADD('month', -24, CURRENT_DATE)
            """

            response = self._execute_athena_query(query)
            return pd.DataFrame(response)

        except Exception as e:
            logger.error(f"Error loading pool performance from S3: {e}")
            return self._generate_mock_pool_performance()

    def _load_risk_scores_from_s3(self) -> pd.DataFrame:
        """Load risk scores from S3"""
        try:
            query = f"""
            SELECT *
            FROM {self.athena_database}.risk_scores
            WHERE score_date >= DATE_ADD('month', -6, CURRENT_DATE)
            ORDER BY score_date DESC
            """

            response = self._execute_athena_query(query)
            return pd.DataFrame(response)

        except Exception as e:
            logger.error(f"Error loading risk scores from S3: {e}")
            return self._generate_mock_risk_scores()

    def _execute_athena_query(self, query: str) -> List[Dict]:
        """Execute Athena query and return results"""
        try:
            # Start query execution
            response = self.athena_client.start_query_execution(
                QueryString=query,
                QueryExecutionContext={'Database': self.athena_database},
                ResultConfiguration={
                    'OutputLocation': f's3://{self.s3_bucket}/athena-results/'
                }
            )

            query_execution_id = response['QueryExecutionId']

            # Wait for query to complete
            import time
            max_attempts = 30
            attempt = 0

            while attempt < max_attempts:
                query_status = self.athena_client.get_query_execution(
                    QueryExecutionId=query_execution_id
                )
                status = query_status['QueryExecution']['Status']['State']

                if status == 'SUCCEEDED':
                    break
                elif status in ['FAILED', 'CANCELLED']:
                    raise Exception(f"Query {status}: {query_status['QueryExecution']['Status'].get('StateChangeReason', '')}")

                time.sleep(2)
                attempt += 1

            if attempt >= max_attempts:
                raise Exception("Query timeout")

            # Get query results
            result = self.athena_client.get_query_results(
                QueryExecutionId=query_execution_id
            )

            # Parse results
            columns = [col['Label'] for col in result['ResultSet']['ResultSetMetadata']['ColumnInfo']]
            rows = []

            for row in result['ResultSet']['Rows'][1:]:  # Skip header row
                values = [col.get('VarCharValue', None) for col in row['Data']]
                rows.append(dict(zip(columns, values)))

            return rows

        except Exception as e:
            logger.error(f"Athena query failed: {e}")
            raise
